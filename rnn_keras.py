# -*- coding: utf-8 -*-
"""RNN_keras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mDeinqvPUOuvl0cWrDvHTa3R2S85e4nr

# 自然語言處理&RNN預測情緒 on IMDb影評

## 載入IMDb資料集
"""

import urllib.request
import os
import tarfile

url = "http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
path = "aclImdb_v1.tar.gz"
if not os.path.isfile(path):
  result = urllib.request.urlretrieve(url,path)
  print("downloaded")

if not os.path.exists("aclImdb"):
  tfile = tarfile.open("aclImdb_v1.tar.gz" , "r:gz")
  result = tfile.extractall('')

"""## 資料前處理"""

from keras.preprocessing import sequence
from keras.preprocessing.text import Tokenizer

import re
def remove_tag(text):
  re_tag = re.compile(r'(<[^>]+>|\.|\,)')
  return re_tag.sub('',text)

import os
def read_files(filetype):
  path = "aclImdb/"
  file_list =[]
  postive_path = path + filetype + "/pos/"
  for f in os.listdir(postive_path):
    file_list +=[postive_path+f]
    
  negtive_path = path + filetype + "/neg/"
  for f in os.listdir(negtive_path):
    file_list +=[negtive_path+f]
  
  all_labels = ([1]*12500 + [0]*12500)
  all_texts =[]
  
  for fi in file_list:
    with open(fi , encoding='utf-8') as file_input:
      all_texts += [remove_tag(" ".join(file_input.readlines()))]
  
  return all_labels , all_texts

y_train , train_text = read_files("train")
y_test , test_text = read_files("test")
o_train_text = train_text
o_test_text = test_text

"""觀察資料格式與**type**"""

print("training data size:%d"%(len(train_text)))
print("testing data size:%d"%(len(test_text)))
print("feature :%s"%(train_text[0]))
print("label :%s"%(y_train[0]))

"""### 建立token 
對於每個word建立一個index
"""

token = Tokenizer(num_words = 2000)
token.fit_on_texts(train_text)

#display word index table
print(token.word_index)

#convert text to vector
x_train_seq = token.texts_to_sequences(train_text)
x_test_seq = token.texts_to_sequences(test_text)

#compare the text and vector
print(train_text[0])
print(x_train_seq[0])

#padding 
x_train = sequence.pad_sequences(x_train_seq , maxlen=100)
x_test = sequence.pad_sequences(x_test_seq , maxlen=100)

#show padding result
print("before length: %d"%(len(x_train_seq[0])))
print("before squence: %s"%(x_train_seq[0]))

print("after length: %d"%(len(x_train[0])))
print("after squence: %s"%(x_train[0]))

"""## build RNN model"""

from keras.models import Sequential
from keras.layers.core import Dense , Dropout , Activation
from keras.layers.embeddings import Embedding
from keras.layers.recurrent import SimpleRNN,LSTM

model = Sequential()
model.add(Embedding(output_dim=32,
                   input_dim=2000,
                   input_length=100))
model.add(Dropout(0.35))
#model.add(SimpleRNN(units=16)) #32*16 + 16 +16*16
model.add(LSTM(units=16)) #4 * (RNN number)
model.add(Dense(units=256,activation='relu')) 
model.add(Dropout(0.35))
model.add(Dense(units=1,activation='sigmoid'))

model.summary()

model.compile(loss='binary_crossentropy',
             optimizer="adam",
             metrics=['accuracy'])

"""## train model"""

train_history = model.fit(x_train , y_train , batch_size=100 ,
                          epochs=10 , verbose=2,
                          validation_split=0.2)

"""## evaluate & check result"""

scores = model.evaluate(x_test , y_test ,verbose=1)
scores[1]

predict = model.predict_classes(x_test).reshape(-1)

SentimentDict = {1:"postive" , 0:"negtive"}
def display_test_Sentiment(i):
  print(test_text[i])
  print("truth:",SentimentDict[y_test[i]],"predict:",SentimentDict[predict[i]])

display_test_Sentiment(1)

"""## predict on real data"""

review = ("This film has been saved by its stars: Ryan Reynolds first and Kevin Costner few minutes later will make you forget most of plot's unbelievable logical holes, awful editing (what's Alice Eve's end?), repetitive scenes and complete lack of bad guy's motive (Jordi Mollà)."+

"And Tommy Lee Jones, Gal Gadot, Gary Oldman and Michael Pitt (Alice Eve here is little more than an extra) strive to fill with workmanship and dignity their otherwise gaunt supporting roles."+

"They all succeed in saving the day and in the end you won't fall asleep or leave theater in contempt. But on your way home you could probably comment that this is one of the most useless (or unconvincing) films you ever watched.")

def preprocessor(text):
  x_test_1_seq = token.texts_to_sequences([text])
  x_test_1_seq_pad = sequence.pad_sequences( x_test_1_seq , maxlen=100)
  return  x_test_1_seq_pad

x_test_1 = preprocessor(review)

predict = model.predict_classes(x_test_1)

print(review)
print("predict:",SentimentDict[predict[0][0]])

"""## 課堂練習
1.修改Tokenizer(num_words = 2000)
<p>
2.修改padding的maxlen 
<p>
3.embedding的output size
<p>
4.用LSTM取代RNN
<p>
5.更改模型的參數及架構 example: RNN units , dropout , epochs , ect
"""